# -*- coding: utf-8 -*-
"""knn_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OUpD2TbrS5xd6Ilj42t8HpkGcdM3W51V
"""

import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn â‰¥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "unsupervised_learning"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

# Ignore useless warnings (see SciPy issue #5998)
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")

'''
from sklearn.datasets import load_digits
import time
start = time.time()
X_digits, y_digits = load_digits(return_X_y=True)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)
'''

from keras.datasets import mnist
import time
start = time.time()

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

n_labeled = 6000

k = 90

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]

#y_representative_digits = np.array([
#    0, 1, 3, 2, 7, 6, 4, 6, 9, 5,
#    1, 2, 9, 5, 2, 7, 8, 1, 8, 6,
#    3, 1, 5, 4, 5, 4, 0, 3, 2, 6,
#    1, 7, 7, 9, 1, 8, 6, 5, 4, 8,
#    5, 3, 3, 6, 7, 9, 7, 8, 4, 9])

y_representative_digits = y_train[representative_digit_idx]
print(y_representative_digits)

y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]

percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]

'''
#from sklearn.linear_model import LogisticRegression
import sklearn.neighbors
model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=25)
#model.fit(X_train_partially_propagated, y_train_partially_propagated)
model.fit(X_train, y_train)
#log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
#log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
'''

#model.score(X_test, y_test)

k_neighbors = 5

#k_clusters = sorted(distances)[:k_neighbors]
'''
dist_ind = kmeans.fit_transform(X_test)
for distances in dist_ind:
  k_clusters = set()
  maxInSet = 1000
  dist_to_ind = {}
  for ind, char in enumerate(distances):
    dist_to_ind[char] = ind
    if char < maxInSet or len(k_clusters) < k_neighbors:
      k_clusters.add(char)
      if len(k_clusters) > k_neighbors:
        k_clusters.remove(maxInSet)
      maxInSet = max(k_clusters)

  print("sdjfalf" + str(k_clusters))

'''

#for k_neighbors in range(1, 10):
import numpy as np
per = 0.0
for i, point in enumerate(X_test):
  k_clusters = set()
  maxInSet = 10000
  k_vals = {}
  for ind, val in enumerate(X_representative_digits):
    dist = np.linalg.norm(val-point)
    if dist < maxInSet or len(k_clusters) < k_neighbors:
      k_vals[dist] = y_representative_digits[ind]
      k_clusters.add(dist)
      if len(k_clusters) > k_neighbors:
        k_vals.pop(maxInSet)
        k_clusters.remove(maxInSet)
      maxInSet = max(k_clusters)
  
  valuess = list(k_vals.values())
  mode = max(set(valuess), key=valuess.count)
  
  if mode == y_test[i]:
    per += 1

print(per/len(X_test))
print(time.time() - start)

'''
X_test_clusters = kmeans.predict(X_test)
per = 0.0
for i, num in enumerate(X_test_clusters):
  if y_representative_digits[num] == y_test[i]:
    per+=1
print(per/len(X_test_clusters))
print(time.time() - start)
'''