# -*- coding: utf-8 -*-
"""kMeans_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13sFzvdkIKuHBd3JEdifN2n2HeENel8SX
"""

import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn â‰¥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "unsupervised_learning"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

# Ignore useless warnings (see SciPy issue #5998)
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")

from sklearn.datasets import make_blobs

blob_centers = np.array(
    [[ 0.2,  2.3],
     [-1.5 ,  2.3],
     [-2.8,  1.8],
     [-2.8,  2.8],
     [-2.8,  1.3]])
blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])

X, y = make_blobs(n_samples=2000, centers=blob_centers,
                  cluster_std=blob_std, random_state=7)

def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)
    plt.xlabel("$x_1$", fontsize=14)
    plt.ylabel("$x_2$", fontsize=14, rotation=0)

plt.figure(figsize=(8, 4))
plot_clusters(X)
save_fig("blobs_plot")
plt.show()

from sklearn.cluster import KMeans

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)

y_pred

y_pred is kmeans.labels_

kmeans.cluster_centers_

kmeans.labels_

X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)

#from sklearn.datasets import load_digits

#X_digits, y_digits = load_digits(return_X_y=True)

#from sklearn.model_selection import train_test_split

#X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)

from keras.datasets import mnist
import time
start = time.time()

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

from sklearn.linear_model import LogisticRegression

n_labeled = 6000

#log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", random_state=42)
#log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
#log_reg.score(X_test, y_test)

k = 150

kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]

plt.figure(figsize=(8, 2))
for index, X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10, 10, index + 1)
    plt.imshow(X_representative_digit.reshape(8, 8), cmap="binary", interpolation="bilinear")
    plt.axis('off')

save_fig("representative_images_diagram", tight_layout=False)
plt.show()

#y_representative_digits = np.array([
#    0, 1, 3, 2, 7, 6, 4, 6, 9, 5,
#    1, 2, 9, 5, 2, 7, 8, 1, 8, 6,
#    3, 1, 5, 4, 5, 4, 0, 3, 2, 6,
#    1, 7, 7, 9, 1, 8, 6, 5, 4, 8,
#    5, 3, 3, 6, 7, 9, 7, 8, 4, 9])

y_representative_digits = y_train[representative_digit_idx]
print(y_representative_digits)

y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]

percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]

#from sklearn.linear_model import LogisticRegression

#log_reg = LogisticRegression(multi_class="ovr", solver="lbfgs", max_iter=5000, random_state=42)
#log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)

#log_reg.score(X_test, y_test)

print(kmeans.predict(X_test))
print(kmeans.labels_)
print(y_test)

X_test_clusters = kmeans.predict(X_test)
per = 0.0
for i, num in enumerate(X_test_clusters):
  if y_representative_digits[num] == y_test[i]:
    per+=1
print(per/len(X_test_clusters))
print(time.time() - start)